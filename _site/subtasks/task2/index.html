<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Task 2: Explainable QE &middot; WMT QE Shared Task
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body class="theme-base-0d">

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          WMT QE Shared Task
        </a>
      </h1>
      <p class="lead">Automatic methods for estimating the quality of MT output at run-time.</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>
      

      
      
        
          
        
      
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/subtasks/previous/">Previous Editions</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/subtasks/resources/">Additional training resources</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/subtasks/task1/">Task 1: Quality prediction</a>
          
        
      
        
          
            <a class="sidebar-nav-item active" href="/subtasks/task2/">Task 2: Explainable QE</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/subtasks/task3/">Task 3: Critical Error Detection</a>
          
        
      
        
      
        
      
      
      <a class="sidebar-nav-item" href="https://github.com/WMT-QE-Task/">GitHub project</a>
    </nav>

    <p>&copy; 2022. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="page">
  <h1 class="page-title">Task 2: Explainable QE</h1>
  <p>In this subtask, we propose to address translation error identification as rationale extraction. Instead of training a dedicated word-level model, the goal is to infer translation errors as an explanation for sentence-level quality scores. In particular, for each pair of source and target sentences, participating teams are asked to provide (1) a sentence-level score estimating the translation quality and (2) a list of continuous token-level scores where the tokens with the highest scores are expected to correspond to translation errors considered relevant by human annotators.</p>

<p>For the explainable QE subtask this year, we will use the same language pairs used in <a href="/subtasks/task1/">Task 1</a>:</p>

<ul>
  <li>English-Russian (En-Ru)</li>
  <li>English-German (En-De)</li>
  <li>Chinese-English (Zh-En)</li>
  <li>English-Marathi (En-Mr)</li>
  <li>English-Czech (En-Cs)</li>
  <li>English-Japanese (En-Ja)</li>
  <li>Khmer-English (Km-En)</li>
  <li>Pashto-English (Ps-En)</li>
</ul>

<p>For each language pair, the participants can use the sentence-level scores to train their QE systems, available <a href="https://github.com/WMT-QE-Task/wmt-qe-2022-data/tree/main/sentence-level-subtask">here</a>. (Please see the resources listed in the “<a href="/subtasks/resources/">Additional training resources</a>” section for more details.) As this subtask aims to promote the research in explainability of QE systems, we encourage the participants to use or develop explanation methods which can identify contributions of tokens in the input. <strong>The participants are not allowed to supervise their models with any token-level or word-level labels or signals (whether they are from natural or synthetic data) in order to directly predict word-level errors.</strong></p>

<blockquote>
  <h4 id="upcoming"><strong>Upcoming</strong></h4>
  <p>Apart from the language-pairs mentioned above, we will also provide test sets on some <strong>surprise</strong> language-pairs.  Check-back soon!</p>
</blockquote>

<h2 id="submission">Submission</h2>

<p>For each language pair, a submission is a zip file consisting of three files.</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">metadata.txt</code> must have exactly two non-empty lines.
    <ul>
      <li>The first line contains your team name. You might use your CodaLab username as your team name.</li>
      <li>The second line contains a short description (2-3 sentences) of the system you used to generate the results. This description will not be shown to other participants.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">sentence.submission</code> with sentence-level scores, one score per line.</li>
  <li><code class="language-plaintext highlighter-rouge">target.submission</code> with target token-level scores. Each line must contain a sequence of scores separated by white space. The number of scores must correspond to the number of target tokens. These token-level scores must represent the importance of each token towards the sentence-level prediction, where a higher score means the token is more likely to be an error. Note that the scores do not need to be normalized.</li>
</ul>

<h2 id="evaluation">Evaluation</h2>

<p>To evaluate the submitted approaches, we will measure how well the token-level scores provided by the participants correspond with human word-level error annotation. Specific set of evaluation metrics is TBD.</p>

<h2 id="baselines">Baselines</h2>

<p>Following the Eval4NLP shared task, we will provide three baselines.</p>
<ul>
  <li>Random explanations</li>
  <li><a href="https://aclanthology.org/2020.wmt-1.122.pdf">TransQuest</a> (as a QE model) + <a href="https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf">LIME</a> (as an explainer)</li>
  <li><a href="https://aclanthology.org/2020.acl-main.151.pdf">XMover</a> (as a QE model) + <a href="https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html">SHAP</a> (as an explainer)</li>
</ul>

<h2 id="recommended-resources">Recommended Resources</h2>

<h3 id="quality-estimation-systems">Quality Estimation Systems</h3>
<ul>
  <li><a href="https://github.com/TharinduDR/TransQuest">TransQuest (Ranasinghe et al. 2020)</a></li>
  <li><a href="https://github.com/sheffieldnlp/deepQuest-py">deepQuest-py (Alva-Manchego et al., 2021)</a></li>
  <li><a href="https://github.com/Unbabel/OpenKiwi">OpenKiwi</a></li>
</ul>

<h3 id="post-hoc-explainability-tools">Post-Hoc Explainability Tools</h3>
<ul>
  <li><a href="https://github.com/marcotcr/lime">LIME (Ribeiro et al., 2016)</a></li>
  <li><a href="https://github.com/slundberg/shap">SHAP (Lundberg and Lee, 2017)</a></li>
  <li><a href="https://captum.ai/">Captum</a></li>
  <li><a href="https://allennlp.org/interpret">AllenNLP Interpret (Wallace et al., 2019)</a></li>
  <li><a href="https://github.com/albermax/innvestigate">iNNvestigate (Alber et al., 2019)</a></li>
</ul>

<h2 id="references">References</h2>

<p>As this subtask is similar to the <a href="https://eval4nlp.github.io/2021/sharedtask.html">explainable QE shared task</a> organized by <a href="https://eval4nlp.github.io/2021/index.html">the Eval4NLP workshop last year</a>, we recommend checking <a href="https://aclanthology.org/2021.eval4nlp-1.17/">their findings paper</a>. Apart from that, please find the list of related papers below.</p>

<ul>
  <li>Christoph Leiter, Piyawat Lertvittayakumjorn, Marina Fomicheva, Wei Zhao, Yang Gao, Steffen Eger (2022). Towards Explainable Evaluation Metrics for Natural Language Generation</li>
  <li>Scott M. Lundberg, Su-In Lee (2017). A Unified Approach to Interpreting Model Predictions. NIPS2017</li>
  <li>Tharindu Ranasinghe, Constantin Orasan, Ruslan Mitkov (2020). TransQuest at WMT2020: Sentence-Level Direct Assessment</li>
  <li>Tharindu Ranasinghe, Constantin Orasan, Ruslan Mitkov (2020). TransQuest: Translation Quality Estimation with Cross-lingual Transformers</li>
  <li>Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin. (2016). “Why Should I Trust You?”: Explaining the Predictions of Any Classifier</li>
  <li>Wei Zhao, Goran Glavaš, Maxime Peyrard, Yang Gao, Robert West, Steffen Eger. (2020). On the Limitations of Cross-lingual Encoders as Exposed by Reference-Free Machine Translation Evaluation</li>
  <li>Marina Fomicheva, Shuo Sun, Erick Fonseca, Frédéric Blain, Vishrav Chaudhary, Francisco Guzmán, Nina Lopatina, Lucia Specia, André F. T. Martins. (2020) MLQE-PE: A Multilingual Quality Estimation and Post-Editing Dataset</li>
  <li>Mo Yu, Shiyu Chang, Yang Zhang, Tommi Jaakkola (2019). Rethinking Cooperative Rationalization: Introspective Extraction and Complement Control</li>
  <li>Mukund Sundararajan, Ankur Taly, Qiqi Yan (2017). Axiomatic Attribution for Deep Networks</li>
  <li>Eric Wallace, Jens Tuyls, Junlin Wang, Sanjay Subramanian, Matt Gardner, Sameer Singh (2019). AllenNLP Interpret: A Framework for Explaining Predictions of NLP Models</li>
</ul>

</div>

    </div>

  </body>
</html>
